{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.contrib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-072d785cb210>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseq2seq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m#from tensorflow.compat.v1.contrib import seq2seq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.contrib'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#import tensorflow.compat.v1 as tf\n",
    "#tf.disable_v2_behavior()\n",
    "\n",
    "\n",
    "from tensorflow.contrib import seq2seq\n",
    "#from tensorflow.compat.v1.contrib import seq2seq\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "def get_ids(id_list, token_size, batch_size, seq_length):\n",
    "    n_bat = ((token_size-1)//(batch_size*seq_length))\n",
    "    assert n_bat!=0, 'Decrease batch_size or num_steps!'\n",
    "    in_id_l = np.array(id_list[:n_bat*batch_size*seq_length])\n",
    "    target_id_l = np.array(id_list[1:n_bat*batch_size*seq_length+1])\n",
    "    in_id = np.split(in_id_l.reshape(batch_size, -1), n_bat, 1)\n",
    "    target_id = np.split(target_id_l.reshape(batch_size, -1), n_bat, 1)\n",
    "    #return np.array(zip(in_id, target_id)), n_bat\n",
    "    return np.array(list(zip(in_id, target_id))), n_bat\n",
    "\n",
    "class Config(object):\n",
    "    #f = open('tokdict.pickle','r')\n",
    "    with open('tokdict.pickle','rb') as f:\n",
    "        token_dict = pickle.load(f)\n",
    "        dict_size = len(token_dict)\n",
    "    #f.close()\n",
    "    batch_size = 128\n",
    "    seq_len = 64\n",
    "    num_layer = 2\n",
    "    hidden_size = 512\n",
    "    max_epoch = 10\n",
    "    keep_prob = 0.8\n",
    "    lr = 0.003\n",
    "    gen_length = 100\n",
    "\n",
    "class Model(object):\n",
    "    def __init__(self, is_training, \n",
    "                is_new, config, \n",
    "                x_input, y_input, in_state):    \n",
    "        inputs = tf.one_hot(x_input, config.dict_size, dtype=tf.float32)\n",
    "        cell1 = tf.nn.rnn_cell.BasicLSTMCell(config.hidden_size)\n",
    "        cell2 = tf.nn.rnn_cell.BasicLSTMCell(config.hidden_size)\n",
    "        if is_training:\n",
    "            cell1 = tf.nn.rnn_cell.DropoutWrapper(cell1, \n",
    "                                    output_keep_prob=config.keep_prob)\n",
    "            cell2 = tf.nn.rnn_cell.DropoutWrapper(cell2, \n",
    "                                    output_keep_prob=config.keep_prob)\n",
    "        layers = [cell1, cell2]\n",
    "        \n",
    "        cell = tf.nn.rnn_cell.MultiRNNCell(layers)\n",
    "        n_s = tf.unstack(in_state, axis=0)\n",
    "        state_tuple = tuple(\n",
    "                [tf.nn.rnn_cell.LSTMStateTuple(n_s[idx][0], n_s[idx][1])\n",
    "                    for idx in range(config.num_layer)]\n",
    "                )\n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, inputs, \n",
    "                        initial_state=state_tuple)\n",
    "        logits = tf.contrib.layers.fully_connected(\n",
    "                outputs, config.dict_size, activation_fn=None,\n",
    "                weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\n",
    "                biases_initializer=tf.zeros_initializer())\n",
    "        self._probs = tf.nn.softmax(logits, name='probs')\n",
    "        cost = seq2seq.sequence_loss(logits, y_input, \n",
    "                tf.ones([config.batch_size, config.seq_len]))\n",
    "        self._final_state = tf.identity(state, name='final_state')\n",
    "        self._cost = tf.identity(cost, name='cost')\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(config.lr)\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "        self._train_op = optimizer.apply_gradients(capped_gradients)\n",
    "\n",
    "    @property\n",
    "    def final_state(self):\n",
    "        return self._final_state\n",
    "\n",
    "    @property\n",
    "    def cost(self):\n",
    "        return self._cost\n",
    "\n",
    "    @property\n",
    "    def probs(self):\n",
    "        return self._probs\n",
    "\n",
    "    @property\n",
    "    def train_op(self):\n",
    "        return self._train_op\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = Config()\n",
    "\n",
    "\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        x = tf.placeholder(tf.int32, shape=[None, None], \n",
    "                                                name='input')\n",
    "        y = tf.placeholder(tf.int32, shape=[None, None],\n",
    "                                                name='target')\n",
    "        state = tf.placeholder(tf.float32, \n",
    "                    shape=[config.num_layer, 2, None, None],\n",
    "                                                name='state')\n",
    "        with tf.name_scope('Train'):\n",
    "            with tf.variable_scope('Model', reuse=None):\n",
    "                m_train = Model(True, False, config, x, y, state)\n",
    "                tf.add_to_collection(\"train_op\", m_train.train_op)\n",
    "        with tf.name_scope('Generate'):\n",
    "            with tf.variable_scope('Model', reuse=True):\n",
    "                m_gen = Model(False, True, config, x, y, state)\n",
    "    #f = open('train.pickle', 'r')\n",
    "    f = open('train.pickle', 'rb')\n",
    "    train_id_list = pickle.load(f)\n",
    "    f.close()\n",
    "    token_size = len(train_id_list)\n",
    "    train_batches, tn_bat = get_ids(train_id_list, token_size, \n",
    "                            config.batch_size, config.seq_len)\n",
    "\n",
    "    #f = open('tokdict.pickle', 'r')\n",
    "    f = open('tokdict.pickle', 'rb')\n",
    "    tokdict = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    #f = open('iddict.pickle', 'r')\n",
    "    f = open('iddict.pickle', 'rb')\n",
    "    iddict = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    zero_state = np.zeros((config.num_layer, 2, \n",
    "                        config.batch_size, config.hidden_size))\n",
    "    gen_state = np.zeros((config.num_layer, 2, \n",
    "                        1, config.hidden_size))\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        sess.run(tf.global_variables_initializer())   \n",
    "        for i in range(config.max_epoch):\n",
    "            t_state = zero_state\n",
    "\n",
    "            for j, (input_x, target_y) in enumerate(train_batches):\n",
    "                fetch = [m_train.cost, m_train.final_state, m_train.train_op]\n",
    "                feed_dict = {x:input_x, y:target_y, state:t_state}\n",
    "                train_loss, t_state, _ = sess.run(fetch, feed_dict) \n",
    "                cur_step = i*tn_bat+j\n",
    "                if cur_step%200==0 or cur_step==config.max_epoch*tn_bat:\n",
    "                    print ('Step: %d Train Loss: %.3f' % (cur_step, train_loss))\n",
    "                if cur_step%600==0:\n",
    "                    gen_ids = []\n",
    "                    pri_tok = '\\n'\n",
    "                    gen_ids.append(tokdict[pri_tok])\n",
    "                    n_state = gen_state\n",
    "                    for tok in range(config.gen_length):\n",
    "                        input_seq = [[gen_ids[-1]]]\n",
    "                        id_tensor = tf.argmax(m_gen.probs, axis=2)\n",
    "                        feed_dict = {x:input_seq, state:n_state}\n",
    "                        fetch = [id_tensor, m_gen.final_state]\n",
    "                        new_id, n_state = sess.run(fetch, feed_dict)\n",
    "                        gen_ids.append(new_id[-1][-1])\n",
    "                    new_mol = ''\n",
    "                    for tok in gen_ids:\n",
    "                        new_mol += iddict[tok]\n",
    "                    print ('Step: %d New Molecule: %s' % (i*tn_bat+j, new_mol))\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(sess, './model/save')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
